{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---Setup Instructions for Sagemaker\n",
    "\n",
    "Step 1: Run these commands in terminal\n",
    "git clone https://github.com/msieg/deep-music-visualizer.git\n",
    "cd deep-music-visualizer\n",
    "pip install -r requirements.txt\n",
    "\n",
    "Step 2: in terminal\n",
    "conda install -n python3 -c conda-forge librosa\n",
    "conda install -n python3 -c conda-forge moviepy\n",
    "\n",
    "Step 3: in notebook\n",
    "!pip install torch \n",
    "!pip install pytorch_pretrained_biggan\n",
    "\n",
    "Imagenet classes list: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'toimage' from 'scipy.misc' (C:\\Users\\jon_9\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\misc\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-47f6c84cdc44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtoimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'toimage' from 'scipy.misc' (C:\\Users\\jon_9\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\misc\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import moviepy.editor as mpy\n",
    "import random\n",
    "import torch\n",
    "from scipy.misc import toimage\n",
    "from tqdm import tqdm\n",
    "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
    "                                       save_as_images, display_in_terminal)\n",
    "\n",
    "#set model based on resolution\n",
    "def model_resolution(resolution=128):\n",
    "    \"\"\"\n",
    "    set model's resolution, default 128\n",
    "\n",
    "    \"\"\"\n",
    "    model_name='biggan-deep-' + resolution\n",
    "    model = BigGAN.from_pretrained(model_resolution())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def song_duration(duration=30):\n",
    "    \"\"\"\n",
    "    Song duration in seconds, returns \n",
    "\n",
    "    \"\"\"\n",
    "    seconds=duration\n",
    "    frame_lim=int(np.floor(seconds*22050/frame_length/batch_size))\n",
    "\n",
    "    return frame_lim\n",
    "\n",
    "\n",
    "#set pitch sensitivity\n",
    "def sensitivity_pitch(pitch_sensitivity=220):\n",
    "    \"\"\"\n",
    "    INT\n",
    "    Set how quickly images move according to pitch\n",
    "    Default 220\n",
    "\n",
    "    \"\"\"\n",
    "    pitch_sensitivity=(300-pitch_sensitivity) * 512 / frame_length\n",
    "\n",
    "    return pitch_sensitivity\n",
    "\n",
    "\n",
    "#set tempo sensitivity\n",
    "def sensitivity_tempo(tempo_sensitivity=0.25):\n",
    "    \"\"\"\n",
    "    FLOAT between 0 and 1\n",
    "    Set how quickly images morph due to tempo\n",
    "    Default 0.25\n",
    "    \"\"\"\n",
    "    tempo_sensitivity = tempo_sensitivity * frame_length / 512\n",
    "\n",
    "    return tempo_sensitivity\n",
    "\n",
    "\n",
    "frame_length = 512 # can reduce this number to make clearer images or increase to reduce computational load\n",
    "\n",
    "#set batch size  \n",
    "batch_size=32\n",
    "\n",
    "#set use_previous_classes\n",
    "# use_previous_vectors=0  # can be changed to 1 if there are mp3 vector analysis available to use\n",
    "    \n",
    "#set output name\n",
    "outname = \"random.mp4\"\n",
    "\n",
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#set smooth factor\n",
    "def smooth_factor(smooth_factor=20):\n",
    "    if smooth_factor > 1:\n",
    "        smooth_factor=int(smooth_factor * 512 / frame_length)\n",
    "    else:\n",
    "        smooth_factor=smooth_factor\n",
    "\n",
    "    return smooth_factor\n",
    "\n",
    "#get new jitters\n",
    "def new_jitters(jitter):\n",
    "    jitters=np.zeros(128)\n",
    "    for j in range(128):\n",
    "        if random.uniform(0,1)<0.5:\n",
    "            jitters[j]=1\n",
    "        else:\n",
    "            jitters[j]=1-jitter        \n",
    "    return jitters\n",
    "\n",
    "\n",
    "#get new update directions\n",
    "def new_update_dir(nv2,update_dir):\n",
    "    for ni,n in enumerate(nv2):                  \n",
    "        if n >= 2*truncation - tempo_sensitivity:\n",
    "            update_dir[ni] = -1  \n",
    "                        \n",
    "        elif n < -2*truncation + tempo_sensitivity:\n",
    "            update_dir[ni] = 1   \n",
    "    return update_dir\n",
    "\n",
    "\n",
    "#smooth class vectors\n",
    "def smooth(class_vectors, smooth_factor = smooth_factor()):\n",
    "    \n",
    "    if smooth_factor==1:\n",
    "        return class_vectors\n",
    "    \n",
    "    class_vectors_terp=[]\n",
    "    for c in range(int(np.floor(len(class_vectors)/smooth_factor)-1)):  \n",
    "        ci=c*smooth_factor          \n",
    "        cva=np.mean(class_vectors[int(ci):int(ci)+smooth_factor],axis=0)\n",
    "        cvb=np.mean(class_vectors[int(ci)+smooth_factor:int(ci)+smooth_factor*2],axis=0)\n",
    "                    \n",
    "        for j in range(smooth_factor):                                 \n",
    "            cvc = cva*(1-j/(smooth_factor-1)) + cvb*(j/(smooth_factor-1))                                          \n",
    "            class_vectors_terp.append(cvc)\n",
    "            \n",
    "    return np.array(class_vectors_terp)\n",
    "\n",
    "\n",
    "#normalize class vector between 0-1\n",
    "def normalize_cv(cv2):\n",
    "    min_class_val = min(i for i in cv2 if i != 0)\n",
    "    for ci,c in enumerate(cv2):\n",
    "        if c==0:\n",
    "            cv2[ci]=min_class_val    \n",
    "    cv2=(cv2-min_class_val)/np.ptp(cv2) \n",
    "    \n",
    "    return cv2\n",
    "\n",
    "def song_analysis(song = None, num_classes = 4, classes = None, truncation = 0.5, jitter = 0.5, depth = 1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        song: 30 second mp3 file\n",
    "        num_classes: INT of how many classes should appear\n",
    "        classes: LIST of classes by index from ImageNet, leave as None for random classes\n",
    "        truncation: FLOAT 0 to 1\n",
    "        jitter: FLOAT 0 to 1\n",
    "        depth: FLOAT 0 to 1\n",
    "\n",
    "    \"\"\"\n",
    "    #read song: audio waveform and sampling rate saved\n",
    "    y, sr = librosa.load(song)\n",
    "\n",
    "    #create spectrogram\n",
    "    spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,fmax=8000, hop_length=frame_length)\n",
    "\n",
    "    #get mean power at each time point\n",
    "    specm=np.mean(spec,axis=0)\n",
    "\n",
    "    #compute power gradient across time points\n",
    "    gradm=np.gradient(specm)\n",
    "\n",
    "    #set max to 1\n",
    "    gradm=gradm/np.max(gradm)\n",
    "\n",
    "    #set negative gradient time points to zero \n",
    "    gradm = gradm.clip(min=0)\n",
    "        \n",
    "    #normalize mean power between 0-1\n",
    "    specm=(specm-np.min(specm))/np.ptp(specm)\n",
    "\n",
    "    #create chromagram of pitches X time points\n",
    "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=frame_length)\n",
    "\n",
    "    #sort pitches by overall power \n",
    "    chromasort=np.argsort(np.mean(chroma,axis=1))[::-1]\n",
    "\n",
    "    if classes==None:\n",
    "        #select 4 random classes\n",
    "        cls1000=list(range(1000))\n",
    "        random.shuffle(cls1000)\n",
    "        classes=cls1000[:4]\n",
    "\n",
    "    print('\\nGenerating input vectors \\n')\n",
    "\n",
    "    #initialize first class vector\n",
    "    cv1=np.zeros(1000)\n",
    "    for pi,p in enumerate(chromasort[:num_classes]):\n",
    "        \n",
    "        if num_classes < 12:\n",
    "            cv1[classes[pi]] = chroma[p][np.min([np.where(chrow>0)[0][0] for chrow in chroma])]       \n",
    "        else:\n",
    "            cv1[classes[p]] = chroma[p][np.min([np.where(chrow>0)[0][0] for chrow in chroma])]\n",
    "\n",
    "    #initialize first noise vector\n",
    "    nv1 = truncated_noise_sample(truncation=truncation)[0]\n",
    "\n",
    "    #initialize list of class and noise vectors\n",
    "    class_vectors=[cv1]\n",
    "    noise_vectors=[nv1]\n",
    "\n",
    "    #initialize previous vectors (will be used to track the previous frame)\n",
    "    cvlast=cv1\n",
    "    nvlast=nv1\n",
    "\n",
    "    #initialize the direction of noise vector unit updates\n",
    "    update_dir=np.zeros(128)\n",
    "    for ni,n in enumerate(nv1):\n",
    "        if n<0:\n",
    "            update_dir[ni] = 1\n",
    "        else:\n",
    "            update_dir[ni] = -1\n",
    "\n",
    "\n",
    "    #initialize noise unit update\n",
    "    update_last=np.zeros(128)\n",
    "\n",
    "    for i in tqdm(range(len(gradm))):   \n",
    "        \n",
    "        #print progress\n",
    "        pass\n",
    "\n",
    "        #update jitter vector every 100 frames by setting ~half of noise vector units to lower sensitivity\n",
    "        if i%200==0:\n",
    "            jitters=new_jitters(jitter)\n",
    "\n",
    "        #get last noise vector\n",
    "        nv1=nvlast\n",
    "\n",
    "        #set noise vector update based on direction, sensitivity, jitter, and combination of overall power and gradient of power\n",
    "        update = np.array([tempo_sensitivity for k in range(128)]) * (gradm[i]+specm[i]) * update_dir * jitters \n",
    "        \n",
    "        #smooth the update with the previous update (to avoid overly sharp frame transitions)\n",
    "        update=(update+update_last*3)/4\n",
    "        \n",
    "        #set last update\n",
    "        update_last=update\n",
    "            \n",
    "        #update noise vector\n",
    "        nv2=nv1+update\n",
    "\n",
    "        #append to noise vectors\n",
    "        noise_vectors.append(nv2)\n",
    "        \n",
    "        #set last noise vector\n",
    "        nvlast=nv2\n",
    "                       \n",
    "        #update the direction of noise units\n",
    "        update_dir=new_update_dir(nv2,update_dir)\n",
    "\n",
    "        #get last class vector\n",
    "        cv1=cvlast\n",
    "        \n",
    "        #generate new class vector\n",
    "        cv2=np.zeros(1000)\n",
    "        for j in range(num_classes):\n",
    "            \n",
    "            cv2[classes[j]] = (cvlast[classes[j]] + ((chroma[chromasort[j]][i])/(pitch_sensitivity)))/(1+(1/((pitch_sensitivity))))\n",
    "\n",
    "        #if more than 6 classes, normalize new class vector between 0 and 1, else simply set max class val to 1\n",
    "        if num_classes > 6:\n",
    "            cv2=normalize_cv(cv2)\n",
    "        else:\n",
    "            cv2=cv2/np.max(cv2)\n",
    "        \n",
    "        #adjust depth    \n",
    "        cv2=cv2*depth\n",
    "        \n",
    "        #this prevents rare bugs where all classes are the same value\n",
    "        if np.std(cv2[np.where(cv2!=0)]) < 0.0000001:\n",
    "            cv2[classes[0]]=cv2[classes[0]]+0.01\n",
    "\n",
    "        #append new class vector\n",
    "        class_vectors.append(cv2)\n",
    "        \n",
    "        #set last class vector\n",
    "        cvlast=cv2\n",
    "\n",
    "    #interpolate between class vectors of bin size [smooth_factor] to smooth frames \n",
    "    class_vectors=smooth(class_vectors,smooth_factor())\n",
    "\n",
    "    #save record of vectors for current video\n",
    "    np.save('class_vectors.npy',class_vectors)\n",
    "    np.save('noise_vectors.npy',noise_vectors)\n",
    "\n",
    "    print('vectors saved')        \n",
    "\n",
    "    return noise_vectors, class_vectors\n",
    "\n",
    "\n",
    "def generate_images(noise_vectors, class_vectors):\n",
    "    \"\"\"\n",
    "    Take vectors from song_analysis and generate images\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #convert to Tensor\n",
    "    noise_vectors = torch.Tensor(np.array(noise_vectors))      \n",
    "    class_vectors = torch.Tensor(np.array(class_vectors))      \n",
    "\n",
    "    #Generate frames in batches of batch_size\n",
    "    print('\\n\\nGenerating frames \\n')\n",
    "\n",
    "    #send to CUDA if running on GPU\n",
    "    model=model_resolution().to(device)\n",
    "    noise_vectors=noise_vectors.to(device)\n",
    "    class_vectors=class_vectors.to(device)\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in tqdm(range(song_duration())):\n",
    "        \n",
    "        #print progress\n",
    "        pass\n",
    "\n",
    "        if (i+1)*batch_size > len(class_vectors):\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "        \n",
    "        #get batch\n",
    "        noise_vector=noise_vectors[i*batch_size:(i+1)*batch_size]\n",
    "        class_vector=class_vectors[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        print('vectors transformed')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "    output_cpu=output.cpu().data.numpy()\n",
    "\n",
    "    #convert to image array and add to frames\n",
    "    for out in output_cpu:    \n",
    "        im=np.array(toimage(out))\n",
    "        frames.append(im)\n",
    "                \n",
    "    #empty cuda cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print('Images generated')\n",
    "\n",
    "    return frames\n",
    "\n",
    "#Save video  \n",
    "def save_video(frames, song):\n",
    "    \"\"\"\n",
    "    Input: frames from generating function, and song mp3\n",
    "\n",
    "    Output: created video in mp4 format\n",
    "\n",
    "    \"\"\"\n",
    "    aud = mpy.AudioFileClip(song, fps = 44100) \n",
    "\n",
    "    clip = mpy.ImageSequenceClip(frames, fps=22050/frame_length)\n",
    "    clip = clip.set_audio(aud)\n",
    "    \n",
    "    \n",
    "    return clip.write_videofile(outname,audio_codec='aac')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
